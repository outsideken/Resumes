{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pymongo\n",
    "import string\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "## NLP\n",
    "from nltk.corpus import treebank, stopwords\n",
    "from nltk import bigrams, trigrams\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "## Database\n",
    "from pymongo import MongoClient\n",
    "\n",
    "## Visualization\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pickleme(temp, filename):\n",
    "    \n",
    "    with open(filename + '.pkl', 'w') as picklefile:\n",
    "        pickle.dump(temp, picklefile)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getpickle(filename):\n",
    "\n",
    "    try:\n",
    "        with open(filename + '.pkl', 'rb') as picklefile:\n",
    "            return pickle.load(picklefile)\n",
    "    except:\n",
    "        return 'There was an error trying to read this file.  Please check the filename or path.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenizenstem(temp, stem = True, nouns = False):\n",
    "    \n",
    "    global stopwords\n",
    "    \n",
    "    temptokens = []\n",
    "    \n",
    "    if nouns:\n",
    "        blob = TextBlob(temp)\n",
    "        tokens = blob.noun_phrases\n",
    "    else:\n",
    "        tokens = nltk.word_tokenize(temp)\n",
    "    \n",
    "    ##  Creates instance of WordNetLemmatizer\n",
    "    wnl = WordNetLemmatizer()\n",
    "    \n",
    "    for word in tokens:\n",
    "        if word.lower() not in stopwords and word not in string.punctuation and len(word) > 2:\n",
    "            if stem:\n",
    "                temptokens.append(wnl.lemmatize(word.lower()))\n",
    "            else:\n",
    "                temptokens.append(word.lower())\n",
    "    \n",
    "    return temptokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getstopwords():\n",
    "\n",
    "    STOP_WORDS = list(getpickle('stopwords'))\n",
    "    stopwords = getpickle('resumestopwords')\n",
    "    \n",
    "    stopwords = [item for sublist in [list(STOP_WORDS) + stopwords] for item in sublist]\n",
    "    \n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addtostoplist(tempword):\n",
    "    \n",
    "    stopwords = getpickle('resumestopwords')\n",
    "    \n",
    "    stopwords.append(tempword)\n",
    "    \n",
    "    stopwords = list(set(stopwords))\n",
    "    \n",
    "    pickleme(stopwords, 'resumestopwords')\n",
    "    \n",
    "    print tempword + ' added to State of the Union stop word list.'\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "was added to State of the Union stop word list.\n"
     ]
    }
   ],
   "source": [
    "addtostoplist('was')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WordCloud from amueller/GitHub\n",
    "\n",
    "* [amueller/GitHub](https://github.com/amueller/word_cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cloud(tokens, maxwords):\n",
    "    \n",
    "    global STOP_WORDS, colors\n",
    "    \n",
    "    wordcloud = WordCloud(height = 500, width = 700, \n",
    "                          background_color = 'white', mode = 'RGBA', \n",
    "                          max_words = maxwords, stopwords = STOP_WORDS,\n",
    "                          margin = 10, random_state = 3).generate(tokens)\n",
    "    plt.figure()\n",
    "\n",
    "    plt.imshow(wordcloud.to_array())\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writes data to JSON file for visualization using mbostock's d3 force directed plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def writejson(df, closest, tempname):\n",
    "\n",
    "    distancedict = defaultdict(list)\n",
    "\n",
    "    for i in range(1,len(df) + 1):\n",
    "        temp = df[['index', i]].sort_values(i)[1:2].values\n",
    "        distancedict[i].append(int(temp[0][0]))\n",
    "        \n",
    "    f = open(tempname + '.json', 'w')\n",
    "    \n",
    "    f.write('{\\n  \"nodes\":[\\n')\n",
    "    \n",
    "    for i in range(1,len(df) + 1):\n",
    "        filename = '0' * (3 - len(str(i))) + str(i)\n",
    "        if i < len(df):\n",
    "            f.write('    {\\\"name\\\":\\\"Resume ' + filename + '\",\\\"group\\\":' + str(distancedict[i][0]) +'},\\n')\n",
    "        else:\n",
    "            f.write('    {\\\"name\\\":\\\"Resume ' + filename + '\",\\\"group\\\":' + str(distancedict[i][0]) +'}\\n')\n",
    "    \n",
    "    f.write('  ],\\n  \\\"links\\\":[\\n')\n",
    "    \n",
    "    for i in range(1,len(df) + 1):\n",
    "        for j in range(closest):\n",
    "            temp = df[['index', i]].sort_values(i)[1+j:2+j].values\n",
    "            if i == int(temp[0][0]) or round(temp[0][1],4) == 0:\n",
    "                j -= 1\n",
    "                continue\n",
    "            if i != len(df):\n",
    "                f.write('    {\\\"source\\\":' + str(i-1) + ',\\\"target\\\":' + str(int(temp[0][0]) - 1) + ',\\\"value\\\":' + str(1) + '},\\n')\n",
    "            else:\n",
    "                f.write('    {\\\"source\\\":' + str(i-1) + ',\\\"target\\\":' + str(int(temp[0][0]) - 1) + ',\\\"value\\\":' + str(1) + '}\\n')\n",
    "    \n",
    "    f.write('  ]\\n}')\n",
    "    f.close()\n",
    "    \n",
    "    print tempname +' JSON file written.'\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "colors = getpickle('tableaucolors')\n",
    "stopwords = getstopwords()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start-up mongoDB and load resume database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service `mongodb` already started, use `brew services restart mongodb` to restart.\r\n"
     ]
    }
   ],
   "source": [
    "! brew services start mongodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ! brew services stop mongodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Initialize MongoDB for use\n",
    "client = MongoClient()\n",
    "db = client['resume_db']\n",
    "resumes = db.resume_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of number of documents: 73\n",
      "Keys in one example document: [u'_id', u'Id', u'Resume']\n"
     ]
    }
   ],
   "source": [
    "print \"Count of number of documents:\", resumes.count()\n",
    "print \"Keys in one example document:\", resumes.find_one().keys()\n",
    "# resumes.find_one() # Uncomment this line to see what a full document looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 73\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "tokens = []\n",
    "\n",
    "for resume in resumes.find({}, {'Resume' : 1, \"_id\": 0}):\n",
    "    corpus.append(resume['Resume'])\n",
    "    tokens.append(tokenizenstem(resume['Resume'], stem = True, nouns = False))\n",
    "\n",
    "print len(corpus), len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the Euclidean Distance between Resumes\n",
    "\n",
    "* [sklearn.feature_extraction.text.TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "* [sklearn.feature_extraction.text.CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
    "* [sklearn.metrics.pairwise.pairwise_distances](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.pairwise_distances.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "euclideancv JSON file written.\n",
      "cosinecv JSON file written.\n",
      "manhattancv JSON file written.\n",
      "cityblockcv JSON file written.\n",
      "l1cv JSON file written.\n",
      "l2cv JSON file written.\n",
      "euclideantfidf JSON file written.\n",
      "cosinetfidf JSON file written.\n",
      "manhattantfidf JSON file written.\n",
      "cityblocktfidf JSON file written.\n",
      "l1tfidf JSON file written.\n",
      "l2tfidf JSON file written.\n"
     ]
    }
   ],
   "source": [
    "nouns = False\n",
    "ngrams = 3\n",
    "\n",
    "mxdf = 0.45\n",
    "mindf = 0.05\n",
    "\n",
    "cvtfidf = [True, False]\n",
    "metrics = ['euclidean', 'cosine', 'manhattan', 'cityblock', 'l1', 'l2']\n",
    "\n",
    "for cv in cvtfidf:\n",
    "\n",
    "    if cv == True:\n",
    "        vect = CountVectorizer(input = 'content', encoding = 'utf-8', decode_error = 'strict', \n",
    "                               strip_accents = None, lowercase=True, preprocessor = None, \n",
    "                               tokenizer = tokenizenstem, stop_words = stopwords,\n",
    "                               token_pattern='(?u)\\b\\w\\w+\\b', ngram_range = (1, ngrams), \n",
    "                               analyzer = 'word', max_df = mxdf, min_df = mindf, max_features = None,\n",
    "                               vocabulary = None, binary = False)    \n",
    "    else:\n",
    "        vect = TfidfVectorizer(input='content', encoding='utf-8', decode_error='strict', \n",
    "                               strip_accents=None, lowercase=True, preprocessor=None, \n",
    "                               tokenizer = tokenizenstem, analyzer='word', stop_words = stopwords, \n",
    "                               token_pattern='(?u)\\b\\w\\w+\\b', ngram_range=(1, ngrams), \n",
    "                               max_df = mxdf, min_df = mindf, max_features=None, vocabulary=None, \n",
    "                               binary=False, norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
    "\n",
    "    wookie = vect.fit_transform(corpus)\n",
    "\n",
    "    for metric in metrics:\n",
    "\n",
    "        jsondata = pairwise_distances(wookie, metric = metric)\n",
    "\n",
    "        df = pd.DataFrame(jsondata, index = np.array(range(1,len(corpus)+1)), \n",
    "                          columns = np.array(range(1,len(corpus)+1)))\n",
    "        \n",
    "        df['index'] = np.array(range(1,len(corpus)+1))\n",
    "\n",
    "        if cv == True:\n",
    "            tempname = metric + 'cv'\n",
    "        else:\n",
    "            tempname = metric + 'tfidf'\n",
    "\n",
    "        writejson(df, 1, tempname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
