{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Gensim Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ipython notebook evaluates the Gensim Word2Vec model.  Word2Vec would have been an interesting method to utilize in evaluating and clustering resumes, but the corpus for this study was of insufficient size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pymongo\n",
    "import seaborn as sns\n",
    "import string\n",
    "\n",
    "## NLP\n",
    "from nltk.corpus import treebank, stopwords\n",
    "from nltk import bigrams, trigrams\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "\n",
    "## Database\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pickleme(temp, filename):\n",
    "    \n",
    "    with open(filename + '.pkl', 'w') as picklefile:\n",
    "        pickle.dump(temp, picklefile)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getpickle(filename):\n",
    "\n",
    "    try:\n",
    "        with open(filename + '.pkl', 'rb') as picklefile:\n",
    "            return pickle.load(picklefile)\n",
    "    except:\n",
    "        return 'There was an error trying to read this file.  Please check the filename or path.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenizenstem(temp, stem = True, nouns = False):\n",
    "    \n",
    "    global stopwords\n",
    "    \n",
    "    temptokens = []\n",
    "    \n",
    "    if nouns:\n",
    "        blob = TextBlob(temp)\n",
    "        tokens = blob.noun_phrases\n",
    "    else:\n",
    "        tokens = nltk.word_tokenize(temp)\n",
    "    \n",
    "    ##  Creates instance of WordNetLemmatizer\n",
    "    wnl = WordNetLemmatizer()\n",
    "    \n",
    "    for word in tokens:\n",
    "        if word.lower() not in stopwords and word not in string.punctuation and len(word) > 2:\n",
    "            if stem:\n",
    "                temptokens.append(wnl.lemmatize(word.lower()))\n",
    "            else:\n",
    "                temptokens.append(word.lower())\n",
    "    \n",
    "    return temptokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getstopwords():\n",
    "\n",
    "    STOP_WORDS = list(getpickle('stopwords'))\n",
    "    stopwords = getpickle('resumestopwords')\n",
    "    \n",
    "    stopwords = [item for sublist in [list(STOP_WORDS) + stopwords] for item in sublist]\n",
    "    \n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addtostoplist(tempword):\n",
    "    \n",
    "    stopwords = getpickle('resumestopwords')\n",
    "    \n",
    "    stopwords.append(tempword)\n",
    "    \n",
    "    stopwords = list(set(stopwords))\n",
    "    \n",
    "    pickleme(stopwords, 'resumestopwords')\n",
    "    \n",
    "    print tempword + ' added to State of the Union stop word list.'\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "colors = getpickle('tableaucolors')\n",
    "stopwords = getstopwords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service `mongodb` already started, use `brew services restart mongodb` to restart.\r\n"
     ]
    }
   ],
   "source": [
    "! brew services start mongodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ! brew services stop mongodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Initialize MongoDB for use\n",
    "client = MongoClient()\n",
    "db = client['resume_db']\n",
    "resumes = db.resume_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of number of documents: 73\n",
      "Keys in one example document: [u'_id', u'Id', u'Resume']\n"
     ]
    }
   ],
   "source": [
    "print \"Count of number of documents:\", resumes.count()\n",
    "print \"Keys in one example document:\", resumes.find_one().keys()\n",
    "# resumes.find_one() # Uncomment this line to see what a full document looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 73\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "tokens = []\n",
    "\n",
    "for resume in resumes.find({}, {'Resume' : 1, \"_id\": 0}):\n",
    "    corpus.append(resume['Resume'])\n",
    "    tokens.append(tokenizenstem(resume['Resume'], stem = True, nouns = False))\n",
    "    \n",
    "print len(corpus), len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim WORD2VEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vect = CountVectorizer(min_df=1, ngram_range=(1, 3), stop_words = stopwords, \n",
    "                       analyzer = 'word', tokenizer = tokenizenstem, strip_accents = None)\n",
    "\n",
    "vect.fit(corpus)\n",
    "x = vect.transform(corpus)\n",
    "x_back = x.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = pd.DataFrame(x_back, columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(tokens, size=100, window=5, min_count=1, workers=4,sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.15180503  0.07556337 -0.05489494 -0.44112751 -0.14328073 -0.02504365\n",
      "  0.14446692  0.19581757  0.17993115 -0.19747534  0.34007633  0.03845399\n",
      "  0.23336329  0.14575806  0.05106496  0.11972934  0.11491957  0.01751838\n",
      " -0.19310498 -0.39826015  0.02164103 -0.30877906  0.19131209 -0.05012842\n",
      "  0.12639877  0.11491076 -0.43837157 -0.04495127 -0.03541079 -0.1174932\n",
      " -0.21457539 -0.18045032 -0.13747507  0.1743906  -0.04182204  0.01783394\n",
      " -0.10643984  0.09667073  0.17975636 -0.17835863 -0.00385763  0.29290849\n",
      " -0.33673871  0.26445672 -0.1941783  -0.22924146 -0.3450107   0.0698831\n",
      " -0.26027599 -0.03138917  0.11805285  0.0379095   0.30093375  0.17852123\n",
      "  0.18834978  0.03420704 -0.08233708  0.01690477 -0.06490424  0.09191529\n",
      " -0.14534323 -0.55950749  0.08534367 -0.12314319  0.01467513  0.06861869\n",
      " -0.24395157 -0.19296676  0.07391211 -0.38301036  0.15676999 -0.35816967\n",
      " -0.33674219  0.02628455  0.11333016  0.17183445  0.39647102  0.16004615\n",
      " -0.3437691   0.28943467  0.091345   -0.2048196  -0.06970879  0.34946096\n",
      "  0.28133374 -0.02421614 -0.32562107 -0.07200038 -0.13915168 -0.00151645\n",
      "  0.41647741 -0.00227418  0.24249808  0.0819746   0.12137499  0.05444548\n",
      " -0.08146625  0.10398822  0.19191542 -0.14507742]\n"
     ]
    }
   ],
   "source": [
    "print model['military']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'staff', 0.9861191511154175),\n",
       " (u'chief', 0.9860691428184509),\n",
       " (u'joint', 0.9857847690582275),\n",
       " (u'liaison', 0.984187126159668),\n",
       " (u'division', 0.9793525338172913)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['command'] ,topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
